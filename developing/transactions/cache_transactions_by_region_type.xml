<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE dita PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<dita>
  <topic id="topic_nlq_sk1_wk">
    <title>Transactions by Region Type</title>
    <body>
      <p>A transaction is managed on a per-cache basis, so multiple regions in the cache can
        participate in a single transaction. The data scope of a <keyword keyref="product_name"/>
        cache transaction is the cache that hosts the transactional data. For partitioned regions,
        this may be a remote host to the one running the transaction application. Any transaction
        that includes one or more partitioned regions is run on the member storing the primary copy
        of the partitioned region data. Otherwise, the transaction host is the same one running the
        application. </p>
        <ul id="ul_yy5_jk1_wk">
          <li>
            <p>The client executing the transaction code is called the transaction initiator.</p>
          </li>
          <li>
            <p>The member contacted by the transaction initiator is called the transaction
              delegate.</p>
          </li>
          <li>
            <p>The member that hosts the data—and the transaction—is called the transaction
              host.</p>
          </li>
        </ul>
      <p>The transaction host may be the same member or different member from the transaction
        initiator. In either case, when the transaction commits, data distribution is done from the
        transaction host in the same way.</p>
      <p>
        <note>If you have consistency checking enabled in your region, the transaction will generate
          all necessary version information for the region update when the transaction commits. See
            <xref href="working_with_transactions.xml#transactions_and_consistency"/> for more
          details.</note>
      </p>
    </body>
  </topic>
  <concept id="concept_ysk_xj1_wk">
    <title>Transactions and Partitioned Regions</title>
    <conbody>
      <p>In partitioned regions, transaction operations are done first on the primary data store
        then distributed to other members from there, regardless of which member initiates the cache
        operation. This is the same as is done for normal cache operations on partitioned regions. </p>
      <p>In this figure, M1 runs two transactions.</p>
        <ul id="ul_889FCAFEE4F9470EAAB472FF24A4FA52">
          <li id="li_DF9EBE26A1284703B61804A54D4200A7">The first transaction, T1, works on data
            whose primary buckets are stored in M1, so M1 is the transaction host. </li>
          <li id="li_AC8D7C1254BE469692700B22C40EF376">The second transaction, T2, works on data
            whose primary buckets are stored in M2, so M1 is the transaction delegate and M2 is the
            transaction host. </li>
        </ul>
      <p><i>Transaction on a Partitioned Region:</i></p>
      <p>
        <image placement="break" id="image_9BF680072A674BCF9F01958753F02952"
          href="../../images_svg/transactions_partitioned_1.svg" align="left"/>
      </p>
      <p>The transaction is managed on the transaction host. This includes the transactional view,
        all operations, and all local cache event handling. In this example, when T2 is committed,
        the data on M2 is updated and the transaction events are distributed throughout the system,
        exactly as if the transaction had originated on M2. </p>
      <p>The first region operation within the transaction determines the transaction host. All
        other operations must also work with that as their transaction host: </p>
        <ul
          id="ul_BE4E509EF82A44889E7DDDA6124DFF2E">
          <li id="li_04A4349715BB4486A252AF8D162BFBF7">All partitioned region data managed inside
            the transaction must use the transaction host as their primary data store. In the
            example, if transaction T2 tried to work on entry W in addition to entries Y and Z, the
              <codeph>TransactionDataNotColocatedException</codeph> would be thrown. For information
            on partitioning data so it is properly colocated for transactions, see <xref
              href="../partitioned_regions/custom_partitioning_and_data_colocation.xml#custom_partitioning_and_data_colocation"
              type="concept" format="dita" scope="local"
              ><?xm-replace_text Understanding Custom Partitioning and Data Colocation?></xref>. In
            addition, the data must not be moved during the transaction. Design partitioned region
            rebalancing to avoid rebalancing while transactions are running. See <xref
              href="../partitioned_regions/rebalancing_pr_data.xml#rebalancing_pr_data"
              type="concept" format="dita" scope="local"
              ><?xm-replace_text Rebalancing Partitioned Region Data?></xref>. </li>
          <li id="li_830704A3C60C4AB490A43348B8050AA0">All non-partitioned region data managed
            inside the transaction must be available on the transaction host and must be
            distributed. Operations on regions with local scope are not allowed in transactions with
            partitioned regions. </li>
        </ul>
      <p>The next figure shows a transaction that operates on two partitioned regions and one
        replicated region. As with the single region example, all local event handling is done on
        the transaction host. </p>
      <p>For a transaction to work, the first operation must be on one of the partitioned regions,
        to establish M2 as the transaction host. Running the first operation on a key in the
        replicated region would set M1 as the transaction host, and subsequent operations on the
        partitioned region data would fail with a
          <codeph>TransactionDataNotColocatedException</codeph> exception. </p>
      <p>
        <i>Transaction on a Partitioned Region with Other Regions:</i>
      </p>
      <p>
        <image placement="break" href="../../images_svg/transactions_partitioned_2.svg"
          id="image_34496249618F46F8B8F7E2D4F342E1E6"/>
      </p>
    </conbody>
  </concept>
  <concept id="concept_nl5_pk1_wk">
    <title>Transactions and Replicated Regions</title>
    <conbody>
      <section id="section_C55E80C7136D4A9A8327563E4B89356D">
        <p>For replicated regions, the transaction and its operations are applied to the local
          member and the resulting transaction state is distributed to other members according to
          the attributes of each region. </p>
        <note>If possible, use <codeph>distributed-ack</codeph> scope for your regions where you
          will run transactions. The <codeph>REPLICATE</codeph> region shortcuts use
            <codeph>distributed-ack</codeph> scope. </note>
        <p>The region’s scope affects how data is distributed during the commit phase. Transactions
          are supported for these region scopes:</p>
          <ul id="ul_3FEEBD3A9B1A4033A3D1B943250E904E">
            <li id="li_4CBDE8D31C7D4D56866841B993C15BD9"><codeph><b>distributed-ack</b></codeph>.
              Handles transactional conflicts both locally and between members. The
                <codeph>distributed-ack</codeph> scope is designed to protect data consistency. This
              scope provides the highest level of coordination among transactions in different
              members. When the commit call returns for a transaction run on all distributed-ack
              regions, you can be sure that the transaction’s changes have already been sent and
              processed. In addition, any callbacks in the remote member have been invoked. </li>
            <li id="li_68A95B28168D4DF2A353F7B88953EB48"><codeph><b>distributed-no-ack</b></codeph>.
              Handles transactional conflicts locally, with less coordination between members. This
              provides the fastest transactions with distributed regions, but it does not work for
              all situations. This scope is appropriate for: <ul
                id="ul_796D59016AF64CCC81B8760B80DAF407">
                <li id="li_C8E4299096654708BC63864F809EEF35">Applications with only one writer </li>
                <li id="li_BA9199984E7340578D9997DC1A8ACC90">Applications with multiple writers that
                  write to nonoverlapping data sets </li>
              </ul>
            </li>
            <li id="li_FFC24384D3F043719EF376DA0F5E1922"><codeph><b>local</b></codeph>. No
              distribution, handles transactional conflicts locally. Transactions on regions with
              local scope have no distribution, but they perform conflict checks in the local
              member. You can have conflict between two threads when their transactions change the
              same entry. </li>
          </ul>
        <p>Transactions on non-replicated regions (regions that use the old API with DataPolicy
          EMPTY, NORMAL and PRELOADED) are always transaction initiators, and the transaction data
          host is always a member with a replicated region. This is similar to the way transactions
          using the PARTITION_PROXY shortcut are forwarded to members with primary bucket. </p>
        <p>
          <note>When you have transactions operating on EMPTY, NORMAL or PARTITION regions, make
            sure that the <keyword keyref="product_name"/> property
              <codeph>conserve-sockets</codeph> is set to false to avoid distributed deadlocks. An
            empty region is a region created with the API
              <codeph>RegionShortcut.REPLICATE_PROXY</codeph> or a region with that uses the old API
            of <codeph>DataPolicy</codeph> set to <codeph>EMPTY</codeph>. </note>
        </p>
      </section>
      <section>
        <title>Conflicting Transactions in Distributed-Ack Regions</title>
        <p>In this series of figures, even after the commit operation is launched, the transaction
          continues to exist during the data distribution (step 3). The commit does not complete
          until the changes are made in the remote caches and M1 receives the acknowledgement that
          verifies that the tasks are complete. </p>
        <p><b>Step 1:</b> Before commit, Transactions T1 and T2 each change the same entry in Region
          B within their local cache. T1 also makes a change to Region A.</p>
          <fig id="fig_jhg_vzj_54">
            <image href="../../images_svg/transactions_replicate_1.svg" id="image_cj1_zzj_54"/>
          </fig>
        <p><b>Step 2:</b> Conflict detected and eliminated. The distributed system recognizes the
          potential conflict from Transactions T1 and T2 using the same entry. T1 started to commit
          first, so it is allowed to continue. T2's commit fails with a conflict.</p><fig
            id="fig_bz1_b1k_54">
            <image href="../../images_svg/transactions_replicate_2.svg" id="image_sbh_21k_54"/>
          </fig>
        <p><b>Step 3:</b> Changes are in transit. T1 commits and its changes are merged into the
          local cache. The commit does not complete until <keyword keyref="product_name"/>
          distributes the changes to the remote regions and acknowledgment is received.</p>
          <fig
            id="fig_jyt_j1k_54">
            <image href="../../images_svg/transactions_replicate_3.svg" id="image_qgl_k1k_54"/>
          </fig>
        <p><b>Step 4:</b> After commit. Region A in M2 and Region B in M3 reflect the changes from
          transaction T1 and M1 has received acknowledgment. Results may not be identical in
          different members if their region attributes (such as expiration) are different.</p>
          <fig
            id="fig_sgr_p1k_54">
            <image href="../../images_svg/transactions_replicate_4.svg" id="image_mkm_q1k_54"/>
          </fig>
      </section>
      <section>
        <title>Conflicting Transactions in Distributed-No-Ack Regions</title>
        <p>These figures show how using the no-ack scope can produce unexpected results. These two
          transactions are operating on the same region B entry. Since they use no-ack scope, the
          conflicting changes cross paths and leave the data in an inconsistent state. </p>
        <p><b>Step 1:</b> As in the previous example, Transactions T1 and T2 each change the same
          entry in Region B within their local cache. T1 also makes a change to Region A. Neither
          commit fails, and the data becomes inconsistent.</p>
          <fig id="fig_d5y_51k_54">
            <image href="../../images_svg/transactions_replicate_1.svg" id="image_jn2_cbk_54"/>
          </fig>
        <p><b>Step 2:</b> Changes are in transit. Transactions T1 and T2 commit and merge their
          changes into the local cache. <keyword keyref="product_name"/> then distributes changes to
          the remote regions.</p>
          <fig id="fig_z3q_2bk_54">
            <image href="../../images_svg/transactions_replicate_no_ack_1.svg" id="image_fk1_hbk_54"
            />
          </fig>
        <p><b>Step 3:</b> Distribution is complete. The non-conflicting changes in Region A have
          been distributed to M2 as expected. For Region B however, T1 and T2 have traded changes,
          which is not the intended result.</p>
          <fig id="fig_vl4_mbk_54">
            <image href="../../images_svg/transactions_replicate_no_ack_2.svg" id="image_ijc_4bk_54"
            />
          </fig>
      </section>
      <section id="section_760DE9F2226B46AD8A025F562CEA4D40">
        <title>Conflicting Transactions with Local Scope</title>
        <p> When encountering conflicts with local scope, the first transaction to start the commit
          process completes, and the other transaction’s commit fails with a conflict.. In the
          diagram below, the resulting value for entry Y depends on which transaction commits
            first.<image placement="break" id="image_A37172C328404796AE1F318068C18F43"
            href="../../images_svg/transactions_replicate_local_1.svg"/>
        </p>
      </section>
    </conbody>
  </concept>
  <concept id="concept_omy_341_wk">
    <title>Transactions and Persistent Regions</title>
    <conbody>
      <p>By default, <keyword keyref="product_name"/> does not allow transactions on persistent
        regions. You can enable the use of transactions on persistent regions by setting the
        property <codeph>gemfire.ALLOW_PERSISTENT_TRANSACTIONS</codeph> to true. This may also be
        accomplished at server startup using
        gfsh:</p>
      <codeblock>gfsh start server --name=server1 --dir=server1_dir \
--J=-Dgemfire.ALLOW_PERSISTENT_TRANSACTIONS=true \</codeblock>
      <p>Since <keyword keyref="product_name"/> does not provide atomic disk persistence guarantees,
        the default behavior is to disallow disk-persistent regions from participating in
        transactions. However, when choosing to enable transactions on persistent regions, consider
        the following:</p>
      <ul id="ul_k4d_x3d_3r">
        <li><keyword keyref="product_name"/> does ensure atomicity for in-memory updates.</li>
        <li>When any failed member is unable to complete the logic triggered by a transaction
          (including subsequent disk writes), that failed member is removed from the distributed
          system and, if restarted, must rebuild its state from surviving nodes that successfully
          complete the updates.</li>
        <li>The chances of multiple nodes failing to complete the disk writes that result from a
          transaction commit due to nodes crashing for unrelated reasons are small. The real risk is
          that the file system buffers holding the persistent updates do not get written to disk in
          the case of operating system or hardware failure. If only the <keyword
            keyref="product_name"/> process crashes, atomicity still exists. The overall risk of
          losing disk updates can also be mitigated by enabling synchronized disk file mode for the
          disk stores, but this incurs a high performance penalty.</li>
      </ul>
      <p>To mitigate the risk of data not get fully written to disk on all copies of the
        participating persistent disk stores:</p>
      <ul id="ul_zkj_dkd_3r">
        <li>Make sure you have enough redundant copies of the data. The guarantees of
          multiple/distributed in-memory copies being (each) atomically updated as part of the
          Transaction commit sequence can help guard against data corruption.</li>
        <li>When executing transactions on persistent regions, we recommend using the
          TransactionWriter to log all transactions along with a time stamp. This will allow you to
          recover in the event that all nodes fail simultaneously while a transaction is being
          committed. You can use the log to recover the data manually.</li>
      </ul>
    </conbody>
  </concept>
</dita>
