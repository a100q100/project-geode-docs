<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/concept.dtd">
<concept id="how_client_server_distribution_works">
	<title>Client-to-Server Event Distribution</title>
	<shortdesc id="shortdesc_133653AD43B143BD9D005CF338A69503">Clients and servers distribute events
		according to client activities and according to interest registered by the client in
		server-side cache changes. </shortdesc>
	<conbody>
		<section id="section_F1070B06B3344D1CA7309934FCE097B9">
			<p>When the client updates its cache, changes to client regions are automatically
				forwarded to the server side. The server-side update is then propagated to the other
				clients that are connected and have subscriptions enabled. The server does not
				return the update to the sending client. </p>
			<p>The update is passed to the server and then passed, with the value, to every other
				client that has registered interest in the entry key. This figure shows how a
				client’s entry updates are propagated. </p>
			<p>
				<image placement="break" id="image_66AB57EEDC154962B32F7951667F4656"
					href="../../images_svg/client_server_event_dist.svg"/>
			</p>
			<p>The figure shows the following process: </p>
				<ol id="ol_34D6F49D07994C039FEE11C487F4C4AC">
					<li id="li_7477E1B0C8304CF1A8E2BA22633C7405">Entry X is updated or created in
						Region A through a direct API call on Client1. </li>
					<li id="li_A7FC1634DC1544578CA4453C3F90AFE3">The update to the region is passed
						to the pool named in the region. </li>
					<li id="li_D21A0A04C697414D9C5E1668E720B470">The pool propagates the event to
						the cache server, where the region is updated. </li>
					<li id="li_B17E37715D484EF99CCEEC4C355C3003">The server member distributes the
						event to its peers and also places it into the subscription queue for
						Client2 because that client has previously registered interest in entry X. </li>
					<li id="li_64B76F248C9943A5BFD8CBB11774CB2A">The event for entry X is sent out
						of the queue to Client2. When this happens is indeterminate. </li>
				</ol>
			<p>Client to server distribution uses the client pool connections to send updates to the
				server. Any region with a named pool automatically forwards updates to the server.
				Client cache modifications pass first through a client <codeph>CacheWriter</codeph>,
				if one is defined, then to the server through the client pool, and then finally to
				the client cache itself. A cache writer, either on the client or server side, may
				abort the operation. </p>
			<p>
				<table id="table_f10288cf-b0b2-4555-92d3-cd0e120b3640">
					<tgroup cols="2">
						<colspec colname="1" colnum="1" colwidth="*"/>
						<colspec colname="2" colnum="2" colwidth="*"/>
						<thead>
							<row>
								<entry>Change in Client Cache </entry>
								<entry>Effect on Server Cache </entry>
							</row>
						</thead>
						<tbody>
							<row>
								<entry>Entry create or update </entry>
								<entry>Creation or update of entry. </entry>
							</row>
							<row>
								<entry>Distributed entry destroy </entry>
								<entry>Entry destroy. The destroy call is propagated to the server
									even if the entry is not in the client cache. </entry>
							</row>
							<row>
								<entry>Distributed region destroy/clear (distributed only) </entry>
								<entry>Region destroy/clear </entry>
							</row>
						</tbody>
					</tgroup>
				</table>
			</p>
			<p>
				<note>Invalidations on the client side are not forwarded to the server. </note>
			</p>
		</section>
		<section id="section_A16562611E094C88B12BC149D5EEEEBA">
			<title>Server-to-Client Event Distribution</title>
			<p>The server automatically sends entry modification events only for keys in which the
				client has registered interest. In the interest registration, the client indicates
				whether to send new values or just invalidations for the server-side entry creates
				and updates. If invalidation is used, the client then updates the values lazily as
				needed. </p>
			<p>This figure shows the complete event subscription event distribution for interest
				registrations, with value receipt requested (receiveValues=true) and without. </p>
			<p>
				<image placement="break" id="image_7FD1450B9D58429F860400801EDFDCAE"
					href="../../images_svg/server_client_event_dist.svg"/>
			</p>
			<p>
				<table id="table_0737376f-b96d-4517-aae9-b9555b6e6d4c">
					<tgroup cols="2">
						<colspec colname="1" colnum="1" colwidth="*"/>
						<colspec colname="2" colnum="2" colwidth="*"/>
						<thead>
							<row>
								<entry>Change in Server Cache </entry>
								<entry>Effect on Client Cache </entry>
							</row>
						</thead>
						<tbody>
							<row>
								<entry>Entry create/update </entry>
								<entry>For subscriptions with <codeph>receiveValues</codeph> set to
									true, entry create or update. <p/>For subscriptions with
										<codeph>receiveValues</codeph> set to false, entry
									invalidate if the entry already exists in the client cache;
									otherwise, no effect. The next client get for the entry is
									forwarded to the server. </entry>
							</row>
							<row>
								<entry>Entry invalidate/destroy (distributed only) </entry>
								<entry>Entry invalidate/destroy </entry>
							</row>
							<row>
								<entry>Region destroy/clear (distributed only) </entry>
								<entry>Region destroy or local region clear </entry>
							</row>
						</tbody>
					</tgroup>
				</table>
			</p>
			<p>Server-side distributed operations are all operations that originate as a distributed
				operation in the server or one of its peers. Region invalidation in the server is
				not forwarded to the client. </p>
			<p>
				<note>To maintain a unified set of data in your servers, do not do local entry
					invalidation in your server regions. </note>
			</p>
		</section>
		<section id="section_34613292788D4FA4AB730045FB9A405A">
			<title>Server-to-Client Message Tracking</title>
			<p>The server uses an asynchronous messaging queue to send events to its clients. Every
				event in the queue originates in an operation performed by a thread in a client, a
				server, or an application in the server’s or some other distributed system. The
				event message has a unique identifier composed of the originating thread’s ID
				combined with its member’s distributed system member ID, and the sequential ID of
				the operation. So the event messages originating in any single thread can be grouped
				and ordered by time from lowest sequence ID to highest. Servers and clients track
				the highest sequential ID for each member thread ID. </p>
			<p>A single client thread receives and processes messages from the server, tracking
				received messages to make sure it does not process duplicate sends. It does this
				using the process IDs from originating threads. </p>
			<p>
				<image placement="break" id="image_F4F9D13252E14F11AD63240AED39191A"
					href="../../images_svg/client_server_message_tracking.svg"/>
			</p>
			<p>The client’s message tracking list holds the highest sequence ID of any message
				received for each originating thread. The list can become quite large in systems
				where there are many different threads coming and going and doing work on the cache.
				After a thread dies, its tracking entry is not needed. To avoid maintaining tracking
				information for threads that have died, the client expires entries that have had no
				activity for more than the <codeph>subscription-message-tracking-timeout</codeph>.
			</p>
		</section>
		<section id="section_99E436C569F3422AA842AA74F73A6B36">
			<title>Client Interest Registration on the Server</title>
			<p>The system processes client interest registration following these steps: </p>
				<ol
					id="ol_25FD52CEDCED4588B2AB572EC0E16542">
					<li id="li_E9E2FE39E7284D319F57FBA58929A5EE">The entries in the client region
						that may be affected by this registration are silently destroyed. Other keys
						are left alone. <ul id="ul_F85EA9BB9603491C853C6A2C42670098">
							<li id="li_899F9D26B652472EBB67097872D05516">For the
									<codeph>registerInterest</codeph> method, the system destroys
								all of the specified keys, leaving other keys in the client region
								alone. So if you have a client region with keys A, B, and C and you
								register interest in the key list A, B, at the start of the
									<codeph>registerInterest</codeph> operation, the system destroys
								keys A and B in the client cache but does not touch key C. </li>
							<li id="li_4605C3FCA6AA44F29B24871BC6AAB3E0">For the
									<codeph>registerInterestRegex</codeph> method, the system
								silently destroys all keys in the client region. </li>
						</ul>
					</li>
					<li id="li_83C729CC40C346A5B7BF50FDF69B7064">The interest specification is sent
						to the server, where it is added to the client’s interest list. The list can
						specify entries that are not in the server region at the time interest is
						registered. </li>
					<li id="li_92A380BAD0F046FD9EE8FB9336716523">If a bulk load is requested in the
						call's <codeph>InterestResultPolicy</codeph> parameter, before control is
						returned to the calling method, the server sends all data that currently
						satisfies the interest specification. The client's region is updated
						automatically with the downloaded data. If the server region is partitioned,
						the entire partitioned region is used in the bulk load. Otherwise, only the
						server’s local cache region is used. The interest results policy options
						are: <ul id="ul_8A02C6A9F7FD4145BF0AAF0AE6398F53">
							<li id="li_953C0C7210E64B70985F5E27E6AA864C">KEYS—The client receives a
								bulk load of all available keys matching the interest registration
								criteria. </li>
							<li id="li_1B587B9196204D6797781C680F98A328">KEYS_VALUES—The client
								receives a bulk load of all available keys and values matching the
								interest registration criteria. This is the default interest result
								policy. </li>
							<li id="li_70A02B1510D2489FBA6E1497D9F489B1">NONE—The client does not
								receive any immediate bulk loading. </li>
						</ul>
					</li>
				</ol>
			<p>Once interest is registered, the server continually monitors region activities and
				sends events to its clients that match the interest. </p>
				<ul
					id="ul_AE2F66BF6D9A4B8585C9916E4D74B8DA">
					<li id="li_F9809DC64CBC4EB5B7A27E1D25E38089">No events are generated by the
						register interest calls, even if they load values into the client
						cache.</li>
					<li id="li_C42BF525A1874B8592ABEBAD5B1BBF12">The server maintains the union of
						all of the interest registrations, so if a client registers interest in key
						‘A’, then registers interest in regular expression "B*", the server will
						send updates for all entries with key ‘A’ or key beginning with the letter
						‘B’. </li>
					<li id="li_E7E34DB2F06B49038F711F44AACBE3F8">The server maintains the interest
						registration list separate from the region. The list can contain
						specifications for entries that are not currently in the server region. </li>
					<li id="li_397FDFD49AEA4BCAAB3C3C29DB9B2815">The
							<codeph>registerInterestRegex</codeph> method uses the standard
							<codeph>java.util.regex</codeph> methods to parse the key specification.
					</li>
				</ul>
		</section>
		<section id="section_928BB60066414BEB9FAA7FB3120334A3">
			<title> Server Failover </title>
			<p>When a server hosting a subscription queue fails, the queueing responsibilities pass
				to another server. How this happens depends on whether the new server is a secondary
				server. In any case, all failover activities are carried out automatically by the
				<keyword keyref="product_name"/> system. </p>
				<ul
					id="ul_B352C337E1684EA59825C247DEB11D7D">
					<li id="li_603021580C0F4FBA86AE46D0C93D8FD3"><b>Non-HA failover:</b> The client
						fails over without high availability if it is not configured for redundancy
						or if all secondaries also fail before new secondaries can be initialized.
						As soon as it can attach to a server, the client goes through an automatic
						reinitialization process. In this process, the failover code on the client
						side silently destroys all entries of interest to the client and refetches
						them from the new server, essentially reinitializing the client cache from
						the new server’s cache. For the notify all configuration, this clears and
						reloads all of the entries for the client regions that are connected to the
						server. For notify by subscription, it clears and reloads only the entries
						in the region interest lists. To reduce failover noise, the events caused by
						the local entry destruction and refetching are blocked by the failover code
						and do not reach the client cache listeners. Because of this, your clients
						could receive some out-of-sequence events during and after a server
						failover. For example, entries that exist on the failed server and not on
						its replacement are destroyed and never recreated during a failover. Because
						the destruction events are blocked, the client ends up with entries removed
						from its cache with no associated destroy events. </li>
					<li id="li_2984417F5D25451D840657535EAA04B9"><b>HA failover:</b> If your client
						pool is configured with redundancy and a secondary server is available at
						the time the primary fails, the failover is invisible to the client. The
						secondary server resumes queueing activities as soon as the primary loss is
						detected. The secondary might resend a few events, which are discarded
						automatically by the client message tracking activities. <p>
							<note>There is a very small potential for message loss during HA server
								failover. The risk is not present for failover to secondaries that
								have fully initialized their subscription queue data. The risk is
								extremely low in healthy systems that use at least two secondary
								servers. The risk is higher in unstable systems where servers often
								fail and where secondaries do not have time to initialize their
								subscription queue data before becoming primaries. To minimize the
								risk, the failover logic chooses the longest-lived secondary as the
								new primary. </note>
						</p>
						<p>
							<note>Redundancy management is handled by the client, so when a durable
								client is disconnected from the server, client event redundancy is
								not maintained. Even if the servers fail one at a time, so that
								running clients have time to fail over and pick new secondary
								servers, an offline durable client cannot fail over. As a result,
								the client loses its queued messages. </note>
						</p>
					</li>
				</ul>
		</section>
	</conbody>
</concept>
