<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/concept.dtd">
<concept id="rec_app_p2p_crash">
	<title>Recovering from Crashes with a Peer-to-Peer Configuration</title>
	<shortdesc>When a member crashes, the remaining members continue operation as though the missing
		application or cache server had never existed. The recovery process differs according to
		region type and scope, as well as data redundancy configuration. </shortdesc>
	<conbody>
		<section id="section_1C54E03359AB4775A9211899A63362A4">
			<p> The other system members are told that it has left unexpectedly. If any remaining
				system member is waiting for a response (ACK), the ACK still succeeds and returns,
				because every member that is still alive has responded. If the lost member had
				ownership of a GLOBAL entry, then the next attempt to obtain that ownership acts as
				if no owner exists. </p>
			<p>Recovery depends on how the member has its cache configured. This section covers the
				following: </p>
				<ul id="ul_D0789BCC44D9466DB25D7893841024A1">
					<li id="li_03309D6FB3F94BD8981B21BB06D2F623">Recovery for Partitioned Regions </li>
					<li id="li_F5F3C080E28D4A0DB3F5FE34562E088C">Recovery for Distributed Regions </li>
					<li id="li_E4101B654AD84328A68CDE3BF0D2EC52">Recovery for Regions of Local Scope </li>
					<li id="li_6B48F5B8E2364016B20B5C0C1EC1B131">Recovering Data From Disk </li>
				</ul>
			<p>To tell whether the regions are partitioned, distributed, or local, check the
					<codeph>cache.xml</codeph> file. If the file contains a local scope setting, the
				region has no connection to any other member: </p>
				<codeblock>&lt;region-attributes scope="local"&gt;</codeblock>
			<p>If the file contains any other scope setting, it is configuring a distributed region.
				For example: </p>
				<codeblock>&lt;region-attributes scope="distributed-no-ack"&gt;</codeblock>
			<p>If the file includes either of the following lines, it is configuring a partitioned
				region. </p>
				<codeblock>&lt;partition-attributes... 
&lt;region-attributes data-policy="partition"/&gt; 
&lt;region-attributes data-policy="persistent-partition"/&gt;</codeblock>
			<p>The reassigned clients continue operating smoothly, as in the failover case. A
				successful rebalancing operation does not create any data loss. </p>
			<p>If rebalancing fails, the client fails over to an active server with the normal
				failover behavior. </p>
		</section>
		<section id="section_0E7D482DD8E84250A10070431B29AAC5">
			<title>Recovery for Partitioned Regions</title>
			<p>When an application or cache server crashes, any data in local memory is lost,
				including any entries in a local partitioned region data store. </p>
			<p>
				<b>Recovery for Partitioned Regions With Data Redundancy</b>
			</p>
			<p>If the partitioned region is configured for redundancy and a member crashes, the
				system continues to operate with the remaining copies of the data. You may need to
				perform recovery actions depending on how many members you have lost and how you
				have configured redundancy in your system.</p>
			<p>By default, <keyword keyref="product_name"/> does not make new copies of the data
				until a new member is brought online to replace the member that crashed. You can
				control this behavior using the recovery delay attributes. For more information, see
					<xref scope="local"
					href="../../developing/partitioned_regions/configuring_ha_for_pr.xml"
					format="dita"/>. </p>
			<p>To recover, start a replacement member. The new member regenerates the lost copies
				and returns them to the configured redundancy level. </p>
			<p>
				<note>Make sure the replacement member has at least as much local memory as the old
					one— the <codeph>local-max-memory</codeph> configuration setting must be the
					same or larger. Otherwise, you can get into a situation where some entries have
					all their redundant copies but others don’t. In addition, until you have
					restarted a replacement member, any code that attempts to create or update data
					mapped to partition region bucket copies (primary and secondary) that have been
					lost can result in an exception. (New transactions unrelated to the lost data
					can fail as well simply because they happen to map to-- or "resolve" to-- a
					common bucketId). </note>
			</p>
			<p>Even with high availability, you can lose data if too many applications and cache
				servers fail at the same time. Any lost data is replaced with new data created by
				the application as it returns to active work. </p>
			<p><i>The number of members that can fail at the same time without losing data is equal
					to the number of redundant copies configured for the region.</i> So if
				redundant-copies=1, then at any given time only one member can be down without data
				loss. If a second goes down at the same time, any data stored by those two members
				will be lost. </p>
			<p>You can also lose access to all copies of your data through network failure. See
					<xref href="recovering_from_network_outages.xml#rec_network_crash"
					type="concept" format="dita" scope="local"/>. </p>
			<p>
				<b>Recovery Without Data Redundancy</b>
			</p>
			<p>If a member crashes and there are no redundant copies, any logic that tries to
				interact with the bucket data is <i>blocked</i> until the primary buckets are
				restored from disk. (If you do not have persistence enabled, <keyword
					keyref="product_name"/> will reallocate the buckets on any available remaining
				nodes, however you will need to recover any lost data using external mechanisms.) </p>
			<p>To recover, restart the member. The application returns to active work and
				automatically begins to create new data. </p>
			<p>If the members with the relevant disk stores cannot be restarted, then you will have
				to revoke the missing disk stores manually using gfsh. See <xref
					href="../../tools_modules/gfsh/command-pages/revoke.xml"
				/>.</p>
			<p>
				<b>Maintaining and Recovering Partitioned Region Redundancy</b>
			</p>
			<p>The following alert [ALERT-1] (warning) is generated when redundancy for a
				partitioned region drops: </p>
			<p>Alert: </p>
				<codeblock>[warning 2008/08/26 17:57:01.679 PDT dataStoregemfire5_jade1d_6424 
&lt;PartitionedRegion Message Processor2&gt; tid=0x5c] Redundancy has dropped below 3 
configured copies to 2 actual copies for /partitionedRegion</codeblock>
				<codeblock>[warning 2008/08/26 18:13:09.059 PDT dataStoregemfire5_jade1d_6424 
&lt;DM-MemberEventInvoker&gt; tid=0x1d5] Redundancy has dropped below 3 
configured copies to 1 actual copy for /partitionedRegion</codeblock>
			<p>The following alert [ALERT-2] (warning) is generated when, after creation of a
				partitioned region bucket, the program is unable to find enough members to host the
				configured redundant copies: </p>
			<p>Alert: </p>
				<codeblock>[warning 2008/08/27 17:39:28.876 PDT gemfire_2_4 &lt;RMI TCP Connection(67)-10.80.250.201&gt; 
tid=0x1786] Unable to find sufficient members to host a bucket in the partitioned region. 
Region name = /partitionedregion Current number of available data stores: 1 number 
successfully allocated = 1 number needed = 2 Data stores available: 
[pippin(21944):41927/42712] Data stores successfully allocated: 
[pippin(21944):41927/42712] Consider starting another member</codeblock>
			<p>The following alert [EXCEPTION-1] (warning) and exception is generated when, after
				the creation of a partitioned region bucket, the program is unable to find any
				members to host the primary copy: </p>
			<p>Alert: </p>
				<codeblock>[warning 2008/08/27 17:39:23.628 PDT gemfire_2_4 &lt;RMI TCP Connection(66)-10.80.250.201&gt; 
tid=0x1888] Unable to find any members to host a bucket in the partitioned region. 
Region name = /partitionedregion Current number of available data stores: 0 number 
successfully allocated = 0 number needed = 2 Data stores available: 
[] Data stores successfully allocated: [] Consider starting another member</codeblock>
			<p>Exception: </p>
				<codeblock>com.gemstone.gemfire.cache.PartitionedRegionStorageException: Unable to find any members to 
					host a bucket in the partitioned region.</codeblock>
				<ul id="ul_362D847224564959B1C24EDC7575BE36">
					<li id="li_59CBEA772F624E8AA45BDB105C3014C3">Region name = /partitionedregion </li>
					<li id="li_6D2326A96B794FB3A4DA6115EECBE09B">Current number of available data
						stores: 0 </li>
					<li id="li_8C9B14CD4D754A978E3D0057ADB9B44F">Number successfully allocated = 0;
						Number needed = 2 </li>
					<li id="li_A963C460BE1B4BDEBAFB74C64462A22F">Data stores available: [] </li>
					<li id="li_BF80732F98CD49988E16456362DE5B02">Data stores successfully allocated:
						[] </li>
				</ul>
			<p>Response:</p>
				<ul id="ul_B03716EF279D42C29527B9E20F7615B9">
					<li id="li_B6DCCC9393A44486AE60C8465F90D1D1">Add additional members configured
						as data stores for the partitioned region. </li>
					<li id="li_3A863FCF74FE4D11B78671F3CF32DE27">Consider starting another member.
					</li>
				</ul>
		</section>
		<section id="section_19CFA40F5EE64C4F8062BFBF7A6C1571">
			<title>Recovery for Distributed Regions</title>
			<p>Restart the process. The system member recreates its cache automatically. If
				replication is used, data is automatically loaded from the replicated regions,
				creating an up-to-date cache in sync with the rest of the system. If you have
				persisted data but no replicated regions, data is automatically loaded from the disk
				store files. Otherwise, the lost data is replaced with new data created by the
				application as it returns to active work. </p>
		</section>
		<section id="section_745AB095D1FA48E392F2C1B95DC18090">
			<title>Recovery for Regions of Local Scope</title>
			<p>Regions of local scope have no memory backup, but may have data persisted to disk. If
				the region is configured for persistence, the data remains in the region’s disk
				directories after a crash. The data on disk will be used to initialize the region
				when you restart. </p>
		</section>
		<section id="section_D9202624335D45BFA2FCC55D702125F7">
			<title>Recovering Data from Disk</title>
			<p>When you persist a region, the entry data on disk outlives the region in memory. If
				the member exits or crashes, the data remains in the region’s disk directories. See
					<xref href="../disk_storage/chapter_overview.xml" type="concept"
					format="dita" scope="local"/>. If the same region is created again, this saved
				disk data can be used to initialize the region. </p>
			<p>Some general considerations for disk data recovery: </p>
			<ul id="ul_w2v_fcm_jq">
				<li>Region persistence causes only entry keys and values to be stored to disk.
					Statistics and user attributes are not stored. </li>
				<li>If the application was writing to the disk asynchronously, the chances of data
					loss are greater. The choice is made at the region level, with the
					disk-synchronous attribute.</li>
				<li>When a region is initialized from disk, last modified time is persisted from
					before the member exit or crash. For information on how this might affect the
					region data, see <xref
						href="../../developing/expiration/chapter_overview.xml"
						type="concept" format="dita" scope="local"
						><?xm-replace_text Expiration?></xref>.</li>
			</ul>
			<p>
				<b>Disk Recovery for Disk Writing—Synchronous Mode and Asynchronous Mode</b>
			</p>
			<p>
				<b>Synchronous Mode of Disk Writing</b>
			</p>
			<p>Alert 1: </p>
				<codeblock>DiskAccessException has occured while writing to the disk for region &lt;Region_Name&gt;. 
Attempt will be made to destroy the region locally.</codeblock>
			<p>Alert 2: </p>
				<codeblock>Encountered Exception in destroying the region locally</codeblock>
			<p>Description: </p>
			<p>These are error log-level alerts. Alert 2 is generated only if there was an error in
				destroying the region. If Alert 2 is not generated, then the region was destroyed
				successfully. The message indicating the successful destruction of a region is
				logged at the information level. </p>
			<p>Alert 3: </p>
				<codeblock>Problem in stopping Cache Servers. Failover of clients is suspect</codeblock>
			<p>Description: </p>
			<p>This is an error log-level alert that is generated only if servers were supposed to
				stop but encountered an exception that prevented them from stopping. </p>
			<p>Response: </p>
			<p>The region may no longer exist on the member. The cache servers may also have been
				stopped. Recreate the region and restart the cache servers. </p>
			<p>
				<b>Asynchronous Mode of Disk Writing</b>
			</p>
			<p>Alert 1: </p>
				<codeblock>Problem in Asynch writer thread for region &lt;Region_name&gt;. It will terminate.</codeblock>
			<p>Alert 2: </p>
				<codeblock>Encountered Exception in destroying the region locally</codeblock>
			<p>Description: </p>
			<p>These are error log-level alerts. Alert 2 is generated only if there was an error in
				destroying the region. If Alert 2 is not generated, then the region was destroyed
				successfully. The message indicating the successful destruction of a region is
				logged at the information level. </p>
			<p>Alert 3: </p>
				<codeblock>Problem in stopping Cache Servers. Failover of clients is suspect</codeblock>
			<p>Description: </p>
			<p>This is an error log-level alert that is generated only if servers were supposed to
				stop but encountered an exception that prevented them from stopping. </p>
			<p>Response: </p>
			<p>The region may no longer exist on the member. The cache servers may also have been
				stopped. Recreate the region and restart the cache servers. </p>
		</section>
	</conbody>
</concept>
