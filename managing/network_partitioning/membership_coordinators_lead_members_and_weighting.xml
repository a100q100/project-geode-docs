<?xml version="1.0"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_23C2606D59754106AFBFE17515DF4330">
	<title>Membership Coordinators, Lead Members and Member Weighting</title>
	<shortdesc>Network partition detection uses a designated membership coordinator and a weighting
		system that accounts for a lead member to determine whether a network partition has
		occurred. </shortdesc>
	<conbody>
		<section id="section_7C67F1D30C1645CC8489E481873691D9">
			<title>Membership Coordinators and Lead Members</title>
			<p> The membership coordinator is a member that manages entry and exit of other members
				of the distributed system. With network partition detection enabled, the coordinator
				can be any <keyword keyref="product_name"/> member but locators are preferred. In a
				locator-based system, if all locators are in the reconnecting state, the system
				continues to function, but new members are not be able to join until a locator has
				successfully reconnected. After a locator has reconnected, the reconnected locator
				will take over the role of coordinator. </p>
			<p>When a coordinator is shutting down, it sends out a view that removes itself from
				list and the other members must determine who the new coordinator is. </p>
			<p>The lead member is determined by the coordinator. Any member that has enabled network
				partition detection, is not hosting a locator, and is not an administrator
				interface-only member is eligible to be designated as the lead member by the
				coordinator. The coordinator chooses the longest-lived member that fits the
				criteria. </p>
			<p>The purpose of the lead member role is to provide extra weight. It does not perform
				any specific functionality. </p>
		</section>
		<section id="section_D819DE21928F4D658C132981307447E3">
			<title>Member Weighting System</title>
			<p> By default, individual members are assigned the following weights: <ul
					id="ul_7BF245E57E3D4F2389E02256424D3ECE">
					<li id="li_A108A5612A55415FB8B08F5B6B8B9C9F">Each member has a weight of 10
						except the lead member. </li>
					<li id="li_98562D2373A740199306CF7B12AD9951">The lead member is assigned a
						weight of 15. </li>
					<li id="li_8E73E57819F145B5BC85EBFDCF59D248">Locators have a weight of 3. </li>
				</ul>
			</p>
			<p>You can modify the default weights for specific members by defining the
					<codeph>gemfire.member-weight</codeph> system property upon startup. </p>
			<p>The weights of members prior to the view change are added together and compared to
				the weight of lost members. Lost members are considered members that were removed
				between the last view and the completed send of the view preparation message. If
				membership is reduced by a certain percentage within a single membership view
				change, a network partition is declared. </p>
			<p>The loss percentage threshold is 51 (meaning 51%). Note that the percentage
				calculation uses standard rounding. Therefore, a value of 50.51 is rounded to 51. If
				the rounded loss percentage is equal to or greater than 51%, the membership
				coordinator initiates shut down. </p>
			<p> </p>
		</section>
		<section id="section_53C963D1B2DF417C973A60981E52CDCF">
			<title>Sample Member Weight Calculations</title>
			<p>This section provides some example calculations. </p>
			<p><b>Example 1:</b> Distributed system with 12 members. 2 locators, 10 cache servers
				(one cache server is designated as lead member.) View total weight equals 111. <ul
					id="ul_C8123D9F5F2045B49BE7B165D9572758">
					<li id="li_867F265518C24207B0F03CECED2C91F1">4 cache servers become unreachable.
						Total membership weight loss is 40 (36%). Since 36% is under the 51%
						threshold for loss, the distributed system stays up. </li>
					<li id="li_AFC0E147D0A645588A7D07DC5A5C4CD8">1 locator and 4 cache servers
						(including the lead member) become unreachable. Membership weight loss
						equals 48 (43%). Since 43% is under the 51% threshold for loss, the
						distributed system stays up. </li>
					<li id="li_8698F43EB7F64A82BC3F397BE3C1F21E">5 cache servers (not including the
						lead member) and both locators become unreachable. Membership weight loss
						equals 56 (49%). Since 49% is under the 51% threshold for loss, the
						distributed system stays up. </li>
					<li id="li_F85DB6B6BB4C4A3BA85382BA6519A534">5 cache servers (including the lead
						member) and 1 locator become unreachable. Membership weight loss equals 58
						(52%). Since 52% is greater than the 51% threshold, the coordinator
						initiates shutdown. </li>
					<li id="li_FA83569BA9264847A9211DBA11290995">6 cache servers (not including the
						lead member) and both locators become unreachable. Membership weight loss
						equals 66 (59%). Since 59% is greater than the 51% threshold, the newly
						elected coordinator (a cache server since lo locators remain) will initiate
						shutdown. </li>
				</ul>
			</p>
			<p><b>Example 2:</b> Distributed system with 4 members. 2 cache servers (1 cache server
				is designated lead member), 2 locators. View total weight is 31. <ul
					id="ul_2948F5BC7B8A42139F02AE6450BFECC8">
					<li id="li_2F3A68402EA54577A52A8E7732F00F30">Cache server designated as lead
						member becomes unreachable. Membership weight loss equals 15 or 48%.
						Distributed system stays up. </li>
					<li id="li_482218AB09764A1A9E5E3CCF638FCA0D">Cache server designated as lead
						member and 1 locator become unreachable. Member weight loss equals 18 or
						58%. Membership coordinator initiates shutdown. If the locator that became
						unreachable was the membership coordinator, the other locator is elected
						coordinator and the initiates shutdown. </li>
				</ul>
			</p>
			<p>Even if network partitioning is not enabled, if quorum loss is detected due to
				unresponsive processes, the locator will also log a severe level message to identify
				the failed processes:
				<msgblock>Possible loss of quorum detected due to loss of {0} cache processes: {1}</msgblock>
				where {0} is the number of processes that failed and {1} lists the processes. </p>
			<p>Enabling network partition detection allows only one subgroup to survive a split. The
				rest of the system is disconnected and the caches are closed. </p>
			<p>When a shutdown occurs, the members that are shut down will log the following alert
				message:
				<msgblock>Exiting due to possible network partition event due to loss of {0} cache processes: {1}</msgblock>
				where <codeph>{0}</codeph> is the count of lost members and <codeph>{1}</codeph> is
				the list of lost member IDs. </p>
		</section>
	</conbody>
</concept>
