<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_lw5_mjm_x4">
 <title>Log Collection Utility</title>
 <shortdesc>To aid in the troubleshooting of <keyword keyref="product_name_long"/> issues, you can use the provided log
  collection utility to gather and upload log files and other troubleshooting artifacts. This tool is only supported on Linux machines.</shortdesc>
 <conbody>
  <p>This utility is used to gather log files and other troubleshooting artifacts from a <keyword keyref="product_name"/>
   cluster. </p>
  <p>The tool goes through and collects all files ending with <codeph>.log</codeph>,
    <codeph>.err</codeph>, .<codeph>cfg</codeph>, <codeph>.gfs</codeph>, <codeph>.stack</codeph>,
    <codeph>.xml</codeph>, <codeph>.properties,</codeph> and <codeph>.txt</codeph> from the working
   directories of running <keyword keyref="product_name"/> processes. It also obtains thread dumps for each <keyword keyref="product_name"/> process
   but will not collect heap dumps.</p>
  <p>The collection utility copies all log and artifact files to its host machine and then
   compresses all the files. You should ensure that the machine
   running the utility has sufficient disk space to hold all the collected log and artifact files
   from the cluster. </p>
  <p>In default mode, the tool requires that a <keyword keyref="product_name"/> process is running on each machine where the
   tool is gathering logs and artifact files. If you would like to collect log and artifact files
   from a machine or machines where <keyword keyref="product_name"/> processes are not running, use <i>Static Copy Mode</i>
   by specifying the <codeph>-m</codeph> option and providing a file that lists log and artifact
   file locations. </p>
  <p>The utility is provided in <codeph>$GEMFIRE/tools/LogCollectionUtility</codeph>.</p>
  <section>
   <title>Usage</title>
   <codeblock>java -jar gfe-logcollect.jar -c &lt;company> -o &lt;output dir> [OPTIONS]

Required arguments:
        -c company name to append to output filename
        -o output directory to store all collected log files

Optional arguments:
        -a comma separated list of hosts with no spaces. EG. host1,host2,host3 (defaults to localhost)
        -u username to use to connect via ssh (defaults to current user)
        -i identity file to use for PKI based ssh (defaults to ~/.ssh/id_[dsa|rsa]
        -p prompt for a password to use for ssh connections
        -t ticket number to append to created zip file
        -d don't clean up collected log files after the zip has been created
        -s send the zip file to Pivotal support
        -f ftp server to upload collected logs to.  Defaults to ftp.gemstone.com
        -v print version of this utility
        -h print this help information

Static Copy Mode
        -m &lt;file> Use a file with log locations instead of scanning for logs.
           Entries should be in the format hostname:/log/location</codeblock>
  </section>
  <section>
   <title>Known Limitations</title>
   <p>The following are known limitations with the tool:</p>
   <ol id="ol_ejg_kkm_x4">
    <li>Only supports Linux hosts.</li>
    <li>Requires SSH access between machines.</li>
    <li>Requires that the username be the same for each host that this app scans. For example, you
     can't specify user@host1, anotherUser@host2, etc.</li>
    <li>Requires that SSH access is available across all hosts using either the same password or the
     same public key.</li>
    <li>In order to get stacks using jstack, this process must be ran as the same user who owns the
     <keyword keyref="product_name"/> process.</li>
    <li>Requires 'jps' (typically in $JAVA_HOME/bin) to be in the user's PATH on each machine.</li>
   </ol>
  </section>
 </conbody>
</concept>
