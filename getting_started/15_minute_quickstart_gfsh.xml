<?xml version="1.0"?>
<!DOCTYPE dita PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<dita>
	<topic id="topic_FE3F28ED18E145F787431EC87B676A76">
		<title><keyword keyref="product_name_long"/> in 15 Minutes or Less</title>
		<shortdesc>Need a quick introduction to <keyword keyref="product_name_long"/>? Take this
			brief tour to try out basic features and functionality. </shortdesc>
		<body>
			<section id="section_ECE5170BAD9B454E875F13BEB5762DDD">
				<title>Step 1: Install <keyword keyref="product_name_long"/>.</title>
				<p>See <xref href="installation/install_standalone.xml#concept_0129F6A1D0EB42C4A3D24861AF2C5425"/>
					for instructions.</p>
			</section>
			<section id="section_582F8CBBD99D42F1A55C07591E2E9E9E">
				<title>Step 2: Use gfsh to start a Locator. </title>
				<p>In a terminal window, use the <codeph>gfsh</codeph> command line interface to
					start up a locator. <keyword keyref="product_name_long"/>
					<i>gfsh</i> (pronounced "gee-fish") provides a single, intuitive command-line
					interface from which you can launch, manage, and monitor <keyword
						keyref="product_name_long"/> processes, data, and applications. See <xref
						href="../tools_modules/gfsh/chapter_overview.xml"
					/>. </p>
				<p>The <i>locator</i> is a <keyword keyref="product_name"/> process that tells new, connecting
					members where running members are located and provides load balancing for server
					use. A locator, by default, starts up a JMX Manager, which is used for
					monitoring and managing of a <keyword keyref="product_name"/> cluster. The
					cluster configuration service uses locators to persist and distribute cluster
					configurations to cluster members. See <xref
						href="../configuring/running/running_the_locator.xml"/> and <xref
						href="../configuring/cluster_config/gfsh_persist.xml"/>.</p>
				<ol id="ol_E7093B61F4644E598DB4FF12FCBFC939">
					<li id="li_9E7D31E90C89499D9C8A89A4878B4C45">Create a scratch working directory
						(for example, <codeph>my_gemfire</codeph>) and change directories into it.
							<codeph>gfsh</codeph> saves locator and server working directories and
						log files in this location. </li>
					<li id="li_D0F19947D83145E59172AFB4B59C89B4">Start gfsh by typing <codeph>gfsh</codeph> at the
						command line (or <codeph>gfsh.bat</codeph> if you are using Windows).
						<codeblock>
    _________________________     __
   / _____/ ______/ ______/ /____/ /
  / /  __/ /___  /_____  / _____  /
 / /__/ / ____/  _____/ / /    / /
/______/_/      /______/_/    /_/    v8.2.0

Monitor and Manage GemFire
gfsh&gt;</codeblock>
					</li>
					<li id="li_94D3B7900CBA4F9FA8066C99E636C9C2"> At the <codeph>gfsh</codeph> prompt, type:
						<codeblock>gfsh>start locator --name=locator1
Starting a GemFire Locator in /home/username/my_gemfire/locator1...
.................................
Locator in /home/username/my_gemfire/locator1 on ubuntu.local[10334] as locator1 is currently online.
Process ID: 3529
Uptime: 18 seconds
GemFire Version: 8.2.0
Java Version: 1.8.0_60
Log File: /home/username/my_gemfire/locator1/locator1.log
JVM Arguments: -Dgemfire.enable-cluster-configuration=true -Dgemfire.load-cluster-configuration-from-dir=false 
-Dgemfire.launcher.registerSignalHandlers=true -Djava.awt.headless=true 
-Dsun.rmi.dgc.server.gcInterval=9223372036854775806
Class-Path: /home/username/Pivotal_GemFire_820_b17919_Linux/lib/gemfire.jar:
/home/username/Pivotal_GemFire_820_b17919_Linux/lib/locator-dependencies.jar

Successfully connected to: [host=ubuntu.local, port=1099]

Cluster configuration service is up and running.
</codeblock>
					</li>
				</ol>
			</section>
<!--
      The Steps (in each title) must be renumbered for the M2 release.
-->

			<section id="section_02C79BFFB5334E78A5856AE1EB1F1F84">
				<title>Step 3. Start Pulse.</title>
				<p>Start up the browser-based Pulse monitoring
					tool. Pulse is a Web Application that provides
					a graphical dashboard for monitoring vital, real-time health and performance of
						<keyword keyref="product_name"/> clusters, members, and regions. See <xref
						href="../tools_modules/pulse/chapter_overview.xml"
					/>.
				</p>
					<codeblock>gfsh&gt;start pulse</codeblock>
				<p>This
					command launches Pulse and automatically connects you to the JMX Manager running
					in the Locator. At the Pulse login screen, type in the default username
						<codeph>admin</codeph> and password <codeph>admin</codeph>. </p>
				<p>The Pulse application now displays the locator you just started (locator1): </p>
				<image href="../images/pulse_locator.png" id="image_ign_ff5_t4"/>		
			</section>
			<section id="section_C617BC1C70EB41B8BCA3691D6E3C891A">
				<title>Step 4: Start a server.</title>
				<p>A <keyword keyref="product_name"/> server is a process that runs as a long-lived,
					configurable member of a cluster (also called a <i>distributed system</i>). The
						<keyword keyref="product_name"/> server is used primarily for hosting
					long-lived data regions and for running standard <keyword keyref="product_name"
					/> processes such as the server in a client/server configuration. See <xref
						href="../configuring/running/running_the_cacheserver.xml"/>.</p>
				<p>Start the cache server:</p>
					<codeblock>gfsh&gt;start server --name=server1 --server-port=40411
					</codeblock>
				<p>This commands starts a cache server named "server1" on the specified port of
					40411. </p>
				<p>Observe the changes (new member and server) in Pulse. Try expanding the
					distributed system icon to see the locator and cache server graphically. </p>
			</section>
			<section id="section_3EA12E44B8394C6A9302DF4D14888AF4">
				<title>Step 5: Create a replicated, persistent region.</title>
				<p>In this step you create a region with the <codeph>gfsh</codeph> command line
					utility. Regions are the core building blocks of the <keyword
						keyref="product_name"/> cluster and provide the means for organizing your
					data. The region you create for this exercise employs replication to replicate
					data across members of the cluster and utilizes persistence to save the data to
					disk. See <xref
						href="../basic_config/data_regions/chapter_overview.xml#data_regions"/>.</p>
				<ol id="ol_93628DE0F59D4107AF005B0317084DEC">
					<li id="li_A7FF582BC92D4FC6893BB64DCFE9D5A9">Create a replicated, persistent
						region:
							<codeblock>gfsh>create region --name=regionA --type=REPLICATE_PERSISTENT
Member  | Status
------- | --------------------------------------
server1 | Region "/regionA" created on "server1"
</codeblock><p>Note
							that the region is hosted on server1. </p></li>
					<li id="li_FE49AEA83B3449798F0A79E7EEE278A3">Use the <codeph>gfsh</codeph>
						command line to view a list of the regions in the cluster.
						<codeblock>gfsh>list regions
List of regions
---------------
regionA
</codeblock></li>
					<li>List the members of your cluster. The locator and cache servers you started
						appear in the list:
						<codeblock>gfsh>list members
  Name   | Id
-------- | ---------------------------------------
locator1 | ubuntu(locator1:3529:locator)&lt;v0>:59926
server1  | ubuntu(server1:3883)&lt;v1>:65390
</codeblock>
					</li>
					<li id="li_20DF751B9C1F408BAF9F0D1449095CB1">To view specifics about a region,
						type the following:
						<codeblock>gfsh>describe region --name=regionA
..........................................................
Name            : regionA
Data Policy     : persistent replicate
Hosting Members : server1
                  
Non-Default Attributes Shared By Hosting Members  

 Type  | Name | Value
------ | ---- | -----
Region | size | 0

</codeblock>
					</li>
					<li id="li_1A1D3C3AC94146C1AB2334D6F58BDFA4">In Pulse, click the green cluster
						icon to see all the new members and new regions that you just added to your
						cluster. </li>
				</ol>
				<p>
					<note>Keep this <codeph>gfsh</codeph> prompt open for the next steps. </note>
				</p>
			</section>
			<section>
				<title>Step 6: Manipulate data in the region and demonstrate persistence.</title>
				<p><keyword keyref="product_name_long"/> manages data as key/value pairs. In most
					applications, a Java program adds, deletes and modifies stored data. You can
					also use gfsh commands to add and retrieve data. See <xref
						href="../tools_modules/gfsh/quick_ref_commands_by_area.xml#topic_C7DB8A800D6244AE8FF3ADDCF139DCE4"
					/>.</p>
				<ol id="ol_ofl_rms_s4">
					<li>Run the following <cmdname>put</cmdname> commands to add some data to the
						region:<codeblock>gfsh>put --region=regionA --key="1" --value="one"
Result      : true
Key Class   : java.lang.String
Key         : 1
Value Class : java.lang.String
Old Value   : &lt;NULL>


gfsh>put --region=regionA --key="2" --value="two"
Result      : true
Key Class   : java.lang.String
Key         : 2
Value Class : java.lang.String
Old Value   : &lt;NULL>
</codeblock></li>
					<li>Run the following command to retrieve data from the
							region:<codeblock>gfsh>query --query="select * from /regionA"

Result     : true
startCount : 0
endCount   : 20
Rows       : 2

Result
------
<b>two
one</b></codeblock><p>Note
							that the result displays the values for the two data entries you created
							with the <cmdname>put</cmdname> commands.</p><p>See <xref
								href="../basic_config/data_entries_custom_classes/chapter_overview.xml"
							/>.</p></li>
					<li>Stop the cache server using the following
						command:<codeblock>gfsh>stop server --name=server1
Stopping Cache Server running in /home/username/my_gemfire/server1 on ubuntu.local[40411] as server1...
Process ID: 3883
Log File: /home/username/my_gemfire/server1/server1.log
....
</codeblock>
					</li>
					<li>Restart the cache server using the following
						command:<codeblock>gfsh&gt;start server --name=server1 --server-port=40411
</codeblock></li>
					<li>Run the following command to retrieve data from the region again-- notice
						that the data is still
							available:<codeblock>gfsh>query --query="select * from /regionA"

Result     : true
startCount : 0
endCount   : 20
Rows       : 2

Result
------
<b>two
one</b></codeblock><p>Because
							regionA uses persistence, it writes a copy of the data to disk. When a
							server hosting regionA starts, the data is populated into the cache.
							Note that the result displays the values for the two data entries you
							created prior to stopping the server with the <cmdname>put</cmdname>
							commands.</p><p>See <xref
								href="../basic_config/data_entries_custom_classes/chapter_overview.xml"
							/>.</p><p>See <xref
								href="../basic_config/data_regions/chapter_overview.xml#data_regions"
							/></p></li>
				</ol>
			</section>
			<section>
				<title>Step 7: Examine the effects of replication.</title>
				<p>In this step, you start a second cache server. Because regionA is replicated, the
					data will be available on any server hosting the region. </p>
				<p>See <xref href="../basic_config/data_regions/chapter_overview.xml#data_regions"
					/></p>
				<ol id="ol_mdt_44s_s4">
					<li>Start a second cache
						server:<codeblock>gfsh&gt;start server --name=server2 --server-port=40412</codeblock>
					</li>
					<li>Run the <cmdname>describe region</cmdname> command to view information about
							regionA:<codeblock>gfsh>describe region --name=regionA
..........................................................
Name            : regionA
Data Policy     : persistent replicate
Hosting Members : server1
                  server2

Non-Default Attributes Shared By Hosting Members  

 Type  | Name | Value
------ | ---- | -----
Region | size | 2

</codeblock><p>Note
							that you do not need to create regionA again for server2. The output of
							the command shows that regionA is hosted on both server1 and server2.
							When gfsh starts a server, it requests the configuration from the
							cluster configuration service which then distributes the shared
							configuration to any new servers joining the cluster. </p></li>
					<li>Add a third data
						entry:<codeblock>gfsh>put --region=regionA --key="3" --value="three"
Result      : true
Key Class   : java.lang.String
Key         : 3
Value Class : java.lang.String
Old Value   : &lt;NULL>
</codeblock></li>
					<li>Open the Pulse application (in a Web browser) and observe the cluster
						topology. You should see a locator with two attached servers. Click the
							<uicontrol>Data</uicontrol> tab to view information about regionA.</li>
					<li>Stop the first cache server with the following
						command:<codeblock>gfsh>stop server --name=server1
Stopping Cache Server running in /home/username/my_gemfire/server1 on ubuntu.local[40411] as server1...
Process ID: 4064
Log File: /home/username/my_gemfire/server1/server1.log
....
</codeblock></li>
					<li>Retrieve data from the remaining cache
							server.<codeblock>gfsh>query --query="select * from /regionA"

Result     : true
startCount : 0
endCount   : 20
Rows       : 3

Result
------
<b>two
one
three</b></codeblock><p>Note
							that the data contains 3 entries, including the entry you just added.
						</p></li>
					<li>Add a fourth data
							entry:<codeblock>gfsh>put --region=regionA --key="4" --value="four"
Result      : true
Key Class   : java.lang.String
Key         : 3
Value Class : java.lang.String
Old Value   : &lt;NULL>
</codeblock><p>Note
							that only server2 is running. Because the data is replicated and
							persisted, all of the data is still available. But the new data entry is
							currently only available on server
						2.</p><codeblock>gfsh>describe region --name=regionA
..........................................................
Name            : regionA
Data Policy     : persistent replicate
Hosting Members : server2

Non-Default Attributes Shared By Hosting Members  

 Type  | Name | Value
------ | ---- | -----
Region | size | 4
</codeblock></li>
					<li>Stop the remaining cache
						server:<codeblock>gfsh>stop server --name=server2
Stopping Cache Server running in /home/username/my_gemfire/server2 on ubuntu.local[40412] as server2...
Process ID: 4185
Log File: /home/username/my_gemfire/server2/server2.log
.....
</codeblock></li>
				</ol>
			</section>
			<section>
				<title>Step 8: Restart the cache servers in parallel.</title>
				<p>In this step you restart the cache servers in parallel. Because the data is
					persisted, the data is available when the servers restart. Because the data is
					replicated, you must start the servers in parallel so that they can synchronize
					their data before starting. </p>
				<ol id="ol_j2v_5vv_t4">
					<li>Start server1. Because regionA is replicated and persistent, it needs data
						from the other server to start and waits for the server to
						start:<codeblock>gfsh>start server --name=server1 --server-port=40411
Starting a GemFire Server in /home/username/my_gemfire/server1...
............................................................................
............................................................................
</codeblock>Note
						that if you look in the <filepath>server1.log</filepath> file for the
						restarted server, you will see a log message similar to the
						following:<codeblock>[info 2015/01/14 09:08:13.610 PST server1 &lt;main> tid=0x1] Region /regionA has pot
entially stale data. It is waiting for another member to recover the latest data.
  My persistent id:
  
    DiskStore ID: 8e2d99a9-4725-47e6-800d-28a26e1d59b1
    Name: server1
    Location: /192.168.129.145:/home/username/my_gemfire/server1/.
  
  Members with potentially new data:
  [
    DiskStore ID: 2e91b003-8954-43f9-8ba9-3c5b0cdd4dfa
    Name: server2
    Location: /192.168.129.145:/home/username/my_gemfire/server2/.
  ]
  Use the "gemfire list-missing-disk-stores" command to see all disk stores that 
are being waited on by other members.
</codeblock></li>
					<li>In a second terminal window, change directories to the scratch working directory (for
						example, <codeph>my_gemfire</codeph>) and start
						gfsh:<codeblock>[username@localhost ~/my_gemfire]$ gfsh
    _________________________     __
   / _____/ ______/ ______/ /____/ /
  / /  __/ /___  /_____  / _____  / 
 / /__/ / ____/  _____/ / /    / /  
/______/_/      /______/_/    /_/    v8.2.0

Monitor and Manage GemFire
</codeblock></li>
					<li>Run the following command to connect to the
						cluster:<codeblock>gfsh>connect --locator=localhost[10334]
Connecting to Locator at [host=localhost, port=10334] ..
Connecting to Manager at [host=ubuntu.local, port=1099] ..
Successfully connected to: [host=ubuntu.local, port=1099]
</codeblock></li>
					<li>Start
							server2:<codeblock>gfsh>start server --name=server2 --server-port=40412</codeblock>
						<p>When server2 starts, note that <b>server1 completes its start up</b> in the first gfsh
								window:</p>
						<codeblock>Server in /home/username/my_gemfire/server1 on ubuntu.local[40411] as server1 is currently online.
Process ID: 3402
Uptime: 1 minute 46 seconds
GemFire Version: 8.2.0
Java Version: 1.8.0_60
Log File: /home/username/my_gemfire/server1/server1.log
JVM Arguments: -Dgemfire.default.locators=192.168.129.145[10334] -Dgemfire.use-cluster-configuration=true 
-XX:OnOutOfMemoryError=kill -KILL %p -Dgemfire.launcher.registerSignalHandlers=true 
-Djava.awt.headless=true -Dsun.rmi.dgc.server.gcInterval=9223372036854775806
Class-Path: /home/username/Pivotal_GemFire_820_b17919_Linux/lib/gemfire.jar:
/home/username/Pivotal_GemFire_820_b17919_Linux/lib/server-dependencies.jar
</codeblock></li>
					<li>Verify that the locator and two servers are
						running:<codeblock>gfsh>list members
  Name   | Id
-------- | ---------------------------------------
server2  | ubuntu(server2:3992)&lt;v8>:21507
server1  | ubuntu(server1:3402)&lt;v7>:36532
locator1 | ubuntu(locator1:2813:locator)&lt;v0>:46644
</codeblock></li>
					<li>Run a query to verify that all the data you entered with the
							<cmdname>put</cmdname> commands is
						available:<codeblock>gfsh>query --query="select * from /regionA"

Result     : true
startCount : 0
endCount   : 20
Rows       : 5

Result
------<b>
one
two
four
Three
</b>
NEXT_STEP_NAME : END
</codeblock></li>
					<li>Stop server2 with the following
						command:<codeblock>gfsh>stop server --dir=server2
Stopping Cache Server running in /home/username/my_gemfire/server2 on 192.168.129.145[40412] as server2...
Process ID: 3992
Log File: /home/username/my_gemfire/server2/server2.log
....
</codeblock>
					</li>
					<li>Run a query to verify that all the data you entered with the
							<cmdname>put</cmdname> commands is still
						available:<codeblock>gfsh>query --query="select * from /regionA"

Result     : true
startCount : 0
endCount   : 20
Rows       : 5

Result
------<b>
one
two
four
Three
</b>
NEXT_STEP_NAME : END
</codeblock></li>
				</ol>
			</section>
			<section id="section_E417BEEC172B4E96A92A61DC7601E572">
				<title>Step 9: Shut down the system including your locators.</title>
				<p>To shut down your cluster, do the following: </p>
				<ol id="ol_4941DF79F7384DF39346AF222FC46EA2">
					<li id="li_38EBD1508AFC43A99C801C3C515947E5">In the current
							<codeph>gfsh</codeph> session, stop the cluster:
							<codeblock>gfsh&gt;shutdown --include-locators=true</codeblock><p>See
								<xref
								href="../tools_modules/gfsh/command-pages/shutdown.xml"
							/>. </p>
					</li>
					<li id="li_8DC6EE8843E14C449500A44CBF4091FA">When prompted, type 'Y' to confirm
						the shutdown of the
						cluster.<codeblock>As a lot of data in memory will be lost, including possibly events in queues, 
do you really want to shutdown the entire distributed system? (Y/n): Y
Shutdown is triggered

gfsh>
No longer connected to ubuntu.local[1099].
gfsh>
</codeblock></li>
					<li>Type <codeph>exit</codeph> to quit the gfsh shell.</li>
				</ol>
			</section>
			<section id="section_C8694C6BB07E4430A73DDD72ABB473F1">
				<title>Step 10: What to do next... </title>
				<p>Here are some suggestions on what to explore next with <keyword
						keyref="product_name_long"/>: </p>
				<ul id="ul_D619F2EB20E24191BE20FCFB547CC294">
					<li>Continue reading the next section to learn more about the components and
						concepts that were just introduced.</li>
					<li id="li_E90787BEDA1542E383E3A9FDD34E1BE5">To get more practice using
							<codeph>gfsh</codeph>, see <xref
							href="../tools_modules/gfsh/tour_of_gfsh.xml#concept_0B7DE9DEC1524ED0897C144EE1B83A34"
						/>. </li>
					<li>To learn about the cluster configuration service, see <xref
							href="../configuring/cluster_config/persisting_configurations.xml#task_bt3_z1v_dl"
						/>.</li>
				</ul>
			</section>
		</body>
	</topic>
</dita>
