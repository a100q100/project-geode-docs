<?xml version="1.0"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_3B5E445B19884680900161BDF25E32C9">
	<title>Main Features of <keyword keyref="product_name_long"/></title>
	<shortdesc id="shortdesc_DABFF8298A6B400C8C49EFA91A20BA98">This section summarizes the main
		features and key functionality of <keyword keyref="product_name_long"/>. </shortdesc>
	<conbody>
		<p>This section summarizes <keyword keyref="product_name_long"/> main features and describes
			key functionality. For specific information about functionality that has been added to
				<keyword keyref="product_name"/> in this release, refer to the Release Notes. <ul
				id="ul_7AAFBC35605B461484313A7F85D0497B">
				<li id="li_3089788338C145FAADCB90BAAAC4BF37"><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_7DB1E9BCAD534713954975F768C73E6B"
						type="section" format="dita" scope="local"/>
				</li>
				<li id="li_5B3FA02C2EBA480295FE0152CF2786D9"><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_CF0E3E5C4F884374B8F2F536DD2A375C"
						type="section" format="dita" scope="local"/>
				</li>
				<li id="li_A9109EB84448437199AE7CE06842696F"><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_9C5D669B583646F1B817284EB494DDA7"
						type="section" format="dita" scope="local"/>
				</li>
				<li id="li_249556E121AB44D5909B7E43CA74EA18"><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_EF7A73D35D1241289C9CA19EDDEBE959"
						type="section" format="dita" scope="local"/>
				</li>
				<li id="li_66F66C31C9714AC39588A2B843D19E31"><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_CEB4ABFF83054AF6A47EA2FA09C240B1"
						type="section" format="dita" scope="local"/>
				</li>
				<li id="li_FB5DB7760F49416BA5B401074288F143"><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_86D2B8CC346349F3913209AF87648A02"
						type="section" format="dita" scope="local"/>
				</li>
				<li id="li_BB8BCDCA23FC4D1585F4F69E52C0B6C5"><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_A65B5F0DE8BF4AA6AFF16E3A75D4E0AD"
						type="section" format="dita" scope="local"/>
				</li>
				<li id="li_8C4468FC0F50476F8736F905344CCA93"><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_97CABBFF553647F6BBBC40AA7AF6D4C7"
						type="section" format="dita" scope="local"/>
				</li>
				<li id="li_01C0E4FE9BA340C6AA92DE9B019A3A09"><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_FCB2640F1BED4692A93F9300A41CE70D"
						type="section" format="dita" scope="local"/>
				</li>
				<li id="li_F775553A75E2431AB6078E62E227E7BD"><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_92A444D4B422434EBD5F81D11F32C1C7"
						type="section" format="dita" scope="local"/>
				</li>
				<li><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_577F601BC9854AA6B53CD3440F9B9A6A"
						type="section" format="dita" scope="local"/>
				</li>
				<li id="li_FF4C3B6E26104C4D93186F6FFE22B321"><xref
						href="product_intro.xml#concept_3B5E445B19884680900161BDF25E32C9/section_FF4C3B6E26104C4D93186F6FFE22B321"
						type="section" format="dita" scope="local"/>
				</li>
			</ul>
		</p>
		<section id="section_7DB1E9BCAD534713954975F768C73E6B">
			<title>Feature Summary</title>
			<p><keyword keyref="product_name"/> includes the following features:<ul
					id="ul_0FDE30898F4341CF8E6E3942F934FBDA">
					<li id="li_EB1C5F29A5824A62A06EAF4ECE89B105">Combines redundancy and a "shared
						nothing" persistence architecture to deliver fail-safe reliability and
						performance. </li>
					<li id="li_705F1B7F120D4485BCBDD12922251603">Horizontally scalable to thousands
						of cache members, with multiple cache topologies to meet different
						deployment needs. The cache can be distributed across multiple computers. </li>
					<li id="li_88F7B0A67F26416AA25060B70D293EC9">Asynchronous and synchronous cache
						update propagation. </li>
					<li id="li_05488E4649694A31BA3A84F9A600A327">Delta propagation distributes only
						the difference between old and new versions of an object (delta) instead of
						the entire object, resulting in significant distribution cost savings. </li>
					<li id="li_041E0A4AA04043D9A2F4EFC8D3E0CC4D">Reliable asynchronous event
						notifications and guaranteed message delivery through optimized, low latency
						distribution layer. </li>
					<li id="li_4B732F749D7945E1BDA0672CB23AE8BB">Applications run 4 to 40 times
						faster with no additional hardware. </li>
					<li id="li_E73C34D57FAF4448B7E3A58AB3A974EF">Data awareness and real-time
						business intelligence. If data changes as you retrieve it, you see the
						changes immediately. </li>
					<li id="li_5963D43ADCD342BBA5A6E289DEDB26B3">JTA compliant transaction support. </li>
					<li>Cluster-wide configurations that can be persisted and exported to other
						clusters.</li>
					<li id="li_E3EAC96F3FEA4E1297F226E762BAFDD4">Continuous querying to provide
						active data change notifications. </li>
					<li>Remote cluster management through HTTP.</li>
					<li>REST APIs for REST-enabled application development.</li>
					<li>Rolling upgrade between major version releases.</li>
					<li>WAN replication.</li>
				</ul></p>
		</section>
		<section id="section_CF0E3E5C4F884374B8F2F536DD2A375C">
			<title>High Read-and-Write Throughput</title>
			<p><keyword keyref="product_name"/> uses concurrent main-memory data structures and a
				highly optimized distribution infrastructure to provide more than 10 times the
				read-and-write throughput of traditional disk-based databases. Applications can make
				copies of data dynamically in memory through synchronous or asynchronous replication
				for high read throughput, or partition the data across many <keyword
					keyref="product_name"/> system members to achieve high read-and-write
				throughput. Data partitioning doubles the aggregate throughput if the data access is
				fairly balanced across the entire data set. Linear increase in throughput is limited
				only by the backbone network capacity. </p>
		</section>
		<section id="section_9C5D669B583646F1B817284EB494DDA7">
			<title>Low and Predictable Latency</title>
			<p><keyword keyref="product_name"/>'s optimized caching layer minimizes context switches between
				threads and processes. It manages data in highly concurrent structures to minimize
				contention points. Communication to peer members is synchronous if the receivers can
				keep up, which keeps the latency for data distribution to a minimum. Servers manage
				object graphs in serialized form to reduce the strain on the garbage collector. </p>
			<p><keyword keyref="product_name"/> partitions subscription management (interest
				registration and continuous queries) across server data stores, ensuring that a
				subscription is processed only once for all interested clients. The resulting
				improvements in CPU use and bandwidth utilization improve throughput and reduce
				latency for client subscriptions. </p>
		</section>
		<section id="section_EF7A73D35D1241289C9CA19EDDEBE959">
			<title>High Scalability</title>
			<p><keyword keyref="product_name"/> achieves scalability through dynamic partitioning of
				data across many members and spreading the data load uniformly across the servers.
				For "hot" data, you can configure the system to expand dynamically to create more
				copies of the data. You can also provision application behavior to run in a
				distributed manner in close proximity to the data it needs. </p>
			<p>If you need to support high and unpredictable bursts of concurrent client load, you
				can increase the number of servers managing the data and distribute the data and
				behavior across them to provide uniform and predictable response times. Clients are
				continuously load balanced to the server farm based on continuous feedback from the
				servers on their load conditions. With data partitioned and replicated across
				servers, clients can dynamically move to different servers to uniformly load the
				servers and deliver the best response times. </p>
			<p>You can also improve scalability by implementing asynchronous "write behind" of data
				changes to external data stores like a database. <keyword keyref="product_name"/>
				avoids a bottleneck by queuing all updates in order and redundantly. You can also
				conflate updates and propagate them in batch to the database. </p>
		</section>
		<section id="section_CEB4ABFF83054AF6A47EA2FA09C240B1">
			<title>Continuous Availability</title>
			<p>In addition to guaranteed consistent copies of data in memory, applications can persist data
				to disk on one or more <keyword keyref="product_name"/> members synchronously or
				asynchronously, by using <keyword keyref="product_name"/>'s "shared nothing disk
				architecture". All asynchronous events (store-forward events) are redundantly
				managed in at least two members such that if one server fails, the redundant one
				takes over. All clients connect to logical servers, and the client fails over
				automatically to alternate servers in a group during failures or when servers become
				unresponsive. </p>
		</section>
		<section id="section_86D2B8CC346349F3913209AF87648A02">
			<title>Reliable Event Notifications</title>
			<p>Publish/subscribe systems offer a data distribution service where new events are
				published into the system and routed to all interested subscribers in a reliable
				manner. Traditional messaging platforms focus on message delivery, but often the
				receiving applications need access to related data before they can process the
				event. This often requires them to access a standard database when the event is
				delivered, making the subscriber limited by the speed of the database. </p>
			<p><keyword keyref="product_name"/> offers data and events through a single system. Data
				is managed as objects in one or more distributed data regions, similar to tables in
				a database. Applications simply insert, update, or delete objects in data regions,
				and the platform delivers the object changes to the subscribers. The subscriber
				receiving the event has direct access to the related data in local memory or can
				fetch the data from one of the other members through a single hop. </p>
		</section>
		<section id="section_A65B5F0DE8BF4AA6AFF16E3A75D4E0AD">
			<title>Parallelized Application Behavior on Data Stores</title>
			<p>You can execute application business logic in parallel on the <keyword keyref="product_name"/>
				members. <keyword keyref="product_name"/>'s data-aware function execution service
				permits execution of arbitrary data-dependent application functions on the members
				where the data is partitioned for locality of reference and scale. </p>
			<p>By colocating the relevant data and parallelizing the calculation, you dramatically
				increase overall throughput. More importantly, the calculation latency is inversely
				proportional to the number of members on which it can be parallelized. </p>
			<p>The fundamental premise is to route the function transparently to the application
				that carries the data subset required by the application function and avoid moving
				data around on the network. Application function can be executed on only one member,
				executed in parallel on a subset of members or in parallel across all members. This
				programming model is similar to the popular Map-Reduce model from Google. Data-aware
				function routing is most appropriate for applications that require iteration over
				multiple data items (such as a query or custom aggregation function). </p>
		</section>
		<section id="section_97CABBFF553647F6BBBC40AA7AF6D4C7">
			<title>Shared-Nothing Disk Persistence</title>
			<p>Each <keyword keyref="product_name"/> system member manages data on disk files
				independent of other members. Failures in disks or cache failures in one member do
				not affect the ability of another cache instance to operate safely on its disk
				files. This "shared nothing" persistence architecture allows applications to be
				configured such that different classes of data are persisted on different members
				across the system, dramatically increasing the overall throughput of the application
				even when disk persistence is configured for application objects. </p>
			<p>Unlike a traditional database system, <keyword keyref="product_name"/> does not
				manage data and transaction logs in separate files. All data updates are appended to
				files that are similar to transactional logs of traditional databases. You can avoid
				disk seek times if the disk is not concurrently used by other processes, and the
				only cost incurred is the rotational latency. Even some of the best disk technology
				today takes about 2ms to seek to a track. </p>
		</section>
		<section id="section_FCB2640F1BED4692A93F9300A41CE70D">
			<title>Reduced Cost of Ownership</title>
			<p>You can configure caching in tiers. The client application process can host a cache
				locally (in memory and overflow to disk) and delegate to a cache server farm on
				misses. Even a 30 percent hit ratio on the local cache translates to significant
				savings in costs. The total cost associated with every single transaction comes from
				the CPU cycles spent, the network cost, the access to the database, and intangible
				costs associated with database maintenance. By managing the data as application
				objects, you avoid the additional cost (CPU cycles) associated with mapping SQL rows
				to objects. </p>
		</section>
		<section id="section_92A444D4B422434EBD5F81D11F32C1C7">
			<title>Single-Hop Capability for Client/Server </title>
			<p>Clients can send individual data requests directly to the server holding the data key,
				avoiding multiple hops to locate data that is partitioned. Metadata in the client
				identifies the correct server. This feature improves performance and client access
				to partitioned regions in the server tier. </p>
		</section>
		<section id="section_577F601BC9854AA6B53CD3440F9B9A6A">
			<title>Client/Server Security</title>
			<p><keyword keyref="product_name"/> supports running multiple, distinct users in client
				applications. This feature accommodates installations in which <keyword
					keyref="product_name"/> clients are embedded in application servers, and each
				application server supports data requests from many users. Each user may be
				authorized to access a small subset of data on the servers, as in a customer
				application where each customer can access only their own orders and shipments. Each
				user in the client connects to the server with its own set of credentials, and is
				given its own access authorization to the server cache. </p>
			<p>Client/server communication has increased security against replay attacks. The server
				now sends the client a unique, random identifier with each response, to be used in
				the next client request. Because of the identifier, even a repeated client operation
				call is sent as a unique request to the server. </p>
		</section>
		<section id="section_091A306900D7402CAE5A46B5F9BFD612">
			<title>Multisite Data Distribution</title>
			<p>Scalability problems can result from data sites being spread out geographically
				across a WAN. GemFire offers a novel model to address these topologies, ranging from
				a single peer-to-peer cluster to reliable communications between data centers across
				the WAN. This model allows distributed systems to scale out in an unbounded and
				loosely-coupled fashion without loss of performance, reliability or data
				consistency. </p>
			<p>At the core of this architecture is the gateway sender configuration used for
				distributing region events to a remote site. You can deploy gateway sender instances
				in parallel, which enables GemFire to increase the throughput for distributing
				region events across the WAN. You can also configure gateway sender queues for
				persistence and high availability to avoid data loss in the case of a member
				failure. </p>
		<section id="section_FF4C3B6E26104C4D93186F6FFE22B321">
			<title>Continuous Querying</title>
			<p>In messaging systems like JMS, clients subscribe to topics and queues. Any message
				delivered to a topic is sent to the subscriber. <keyword keyref="product_name"/>
				allows continuous querying by having applications express complex interest using the
				OQL query language. </p>
		</section>
	</conbody>
</concept>
